services:
  syntextaiapp:
    image: osasdeeon/syntextai:latest
    container_name: syntextaiapp
    restart: unless-stopped
    ports:
      - "3000:3000"
    env_file:
      - .env
    environment:
      - WHISPER_CACHE_DIR=/app/models
      - PYTHONUNBUFFERED=1
      # App DB pool (keep low to avoid exhausting managed Postgres connection limits)
      - DB_POOL_SIZE=2
      - DB_MAX_OVERFLOW=1
    # Database connection is handled via environment variables
    volumes:
      - syntextai_models:/app/models
      - ./api/config:/app/api/config
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
          
  worker:
    image: osasdeeon/syntextai:latest
    container_name: syntextai-worker
    command: python -m api.worker
    env_file:
      - .env
    environment:
      - MAX_CONCURRENT_TASKS=1
      - POLL_INTERVAL=10
      - API_BASE_URL=http://syntextaiapp:3000  # ⬅️ Use service name instead of localhost
      # Worker-specific connection pool settings (smaller to avoid exhausting DB connections)
      - DB_POOL_SIZE=1
      - DB_MAX_OVERFLOW=0
    volumes:
      - ./api/config:/app/api/config
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  syntextai_models:
    driver: local

networks:
  default:
    driver: bridge